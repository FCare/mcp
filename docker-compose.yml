version: '3.8'

services:
  joshua-backend:
    build:
      context: .
      dockerfile: Dockerfile.backend
    container_name: joshua-backend
    environment:
      # Configuration du pipeline
      - PIPELINE_NAME=${PIPELINE_NAME:-Pipeline Chat WebSocket}
      - WEBSOCKET_PORT=${WEBSOCKET_PORT:-8768}
      
      # Cl√©s API
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - LLAMACPP_API_KEY=${LLAMACPP_API_KEY}
      - ASR_API_KEY=${ASR_API_KEY}
      - TTS_API_KEY=${TTS_API_KEY}
      
      # Configuration Python
      - PYTHONPATH=/app
      - PYTHONUNBUFFERED=1
      
    ports:
      - "${WEBSOCKET_PORT:-8768}:${WEBSOCKET_PORT:-8768}"
    
    volumes:
      # Logs et fichiers temporaires
      - joshua_logs:/app/logs
      - joshua_temp:/app/temp
      - ./cache:/root/.cache    # Persiste le cache llama.cpp
    
    networks:
      - ansible
    
    # Restart policy
    restart: unless-stopped
    
    # Logs configuration
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  joshua-frontend:
    build:
      context: .
      dockerfile: Dockerfile.frontend
    container_name: joshua-frontend
    ports:
      - "${FRONTEND_PORT:-3000}:80"
    
    depends_on:
      - joshua-backend
    
    networks:
      - ansible
    
    # Restart policy
    restart: unless-stopped
    
    # Logs configuration
    logging:
      driver: json-file
      options:
        max-size: "5m"
        max-file: "3"

volumes:
  joshua_logs:
    driver: local
  joshua_temp:
    driver: local

networks:
  ansible:
    external: true
    driver: bridge
